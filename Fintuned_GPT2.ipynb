{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gl2Z-axAB4Ve"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[sentencepiece]\n",
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import transformers\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer,AutoModel,TFGPT2LMHeadModel,AutoConfig,DataCollatorForLanguageModeling,TFAutoModelForCausalLM,TFAutoModel,GPT2Tokenizer\n",
        "from transformers import DataCollatorWithPadding\n",
        "from datasets import load_dataset\n",
        "from transformers import create_optimizer,pipeline\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from datasets import Dataset, DatasetDict\n",
        "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "vmWqwx2yCI8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=load_dataset('csv',data_files='Pushkin.csv',encoding='latin-1')"
      ],
      "metadata": {
        "id": "QD5676M2DxEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = 128\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "DswhdSpfFyeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# since the instruction in hugging worng and only produce one output for tokenize funtion\n",
        "data_list=[]\n",
        "for element in dataset['train']:\n",
        "    token=tokenizer.encode(element['Content'],add_special_tokens=False)\n",
        "    length=len(token)\n",
        "    for i in range(0,length,context_length):\n",
        "      if i+context_length<=length:\n",
        "        tmp=[]\n",
        "        tmp = token[i:i+context_length]\n",
        "      else:\n",
        "        tmp = pad_sequences([token[i:]], maxlen=context_length, padding='post', truncating='pre',value=50256).flatten().tolist() #50256=eos_token=pad_token\n",
        "\n",
        "      data_list.append(tmp)\n",
        "data_list={'input_ids': data_list}"
      ],
      "metadata": {
        "id": "cj8N81UUH6Nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_list = Dataset.from_dict(data_list)\n",
        "data_list = DatasetDict({\"train\": data_list})\n",
        "data_list = data_list['train'].train_test_split(train_size=0.9,seed=23)"
      ],
      "metadata": {
        "id": "IEY1J1oD3TWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_funtion(element):\n",
        "  return {'input_ids': element['input_ids']}"
      ],
      "metadata": {
        "id": "WexQP3MgygHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_map=data_list.map(map_funtion,batched=True)"
      ],
      "metadata": {
        "id": "THkF5g0lOgrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=TFGPT2LMHeadModel.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "W7WTGtq_Kxm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors=\"tf\")"
      ],
      "metadata": {
        "id": "_S3EeMSlMwFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_train_dataset = model.prepare_tf_dataset(\n",
        "    data_map[\"train\"],\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=True,\n",
        "    batch_size=16,\n",
        ")\n",
        "tf_eval_dataset = model.prepare_tf_dataset(\n",
        "    data_map[\"test\"],\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=False,\n",
        "    batch_size=16,\n",
        ")"
      ],
      "metadata": {
        "id": "kV4I66H2M6mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_loss(label, pred):\n",
        "    mask = label != 50256\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "    loss = loss_object(label, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss *= mask\n",
        "\n",
        "    loss = tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "dLMXMi698Bow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_accuracy(label, pred):\n",
        "    pred = tf.argmax(pred, axis=2)\n",
        "    label = tf.cast(label, pred.dtype)\n",
        "    match = label == pred\n",
        "\n",
        "    mask = label != 50256\n",
        "\n",
        "    match = match & mask\n",
        "\n",
        "    match = tf.cast(match, dtype=tf.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    return tf.reduce_sum(match) / tf.reduce_sum(mask)\n"
      ],
      "metadata": {
        "id": "uLYDqm_GDWRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "isUrTVONSz0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs=20\n",
        "num_train_steps=len(tf_train_dataset)\n",
        "optimizer,schedule=create_optimizer(\n",
        "    init_lr=5e-5,\n",
        "    num_warmup_steps=1000,\n",
        "    num_train_steps=num_train_steps,\n",
        "    weight_decay_rate=0.01\n",
        ")\n",
        "model.compile(optimizer=optimizer,metrics=[masked_accuracy])\n",
        "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
      ],
      "metadata": {
        "id": "_pWzvTceQIUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(tf_train_dataset,epochs=100,validation_data=tf_train_dataset)"
      ],
      "metadata": {
        "id": "tcXVt7EsRWAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\n",
        "    \"text-generation\", model=model, tokenizer=tokenizer, device=0\n",
        ")"
      ],
      "metadata": {
        "id": "3joXtatZU6BH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt='i love you'\n",
        "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "dFdWxqrnU6pz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}