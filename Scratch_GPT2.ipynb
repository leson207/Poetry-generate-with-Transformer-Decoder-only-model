{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gl2Z-axAB4Ve"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[sentencepiece]\n",
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import transformers\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer,AutoModel,TFGPT2LMHeadModel,AutoConfig,DataCollatorForLanguageModeling\n",
        "from datasets import load_dataset\n",
        "from transformers import create_optimizer,pipeline\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import AdamWeightDecay, get_scheduler\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay"
      ],
      "metadata": {
        "id": "vmWqwx2yCI8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=load_dataset('csv',data_files='Pushkin.csv',encoding='latin-1')"
      ],
      "metadata": {
        "id": "QD5676M2DxEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = 128\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "DswhdSpfFyeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sequense_process(x,step,length,sequence):\n",
        "    if x+step<=length:\n",
        "      return sequence[x:x+step]\n",
        "    else:\n",
        "      return pad_sequences([sequence[x:]], maxlen=step, padding='pre', truncating='pre',value=50256).flatten().tolist() #eos_token"
      ],
      "metadata": {
        "id": "u23L9YKNhhco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# since the instruction in hugging worng and only produce one output for tokenize funtion\n",
        "data_list=[]\n",
        "for element in dataset['train']:\n",
        "    token=tokenizer.encode(element['Content'],add_special_tokens=False)\n",
        "    length=len(token)\n",
        "    for i in range(0,length,context_length):\n",
        "      tmp=[]\n",
        "      if i+context_length<=length:\n",
        "        tmp = token[i:i+context_length]\n",
        "      else:\n",
        "        tmp = pad_sequences([token[i:]], maxlen=context_length, padding='post', truncating='pre',value=50256).flatten().tolist() #50256=eos_token=pad_token\n",
        "\n",
        "      data_list.append(tmp)\n",
        "      # data_list.append(sequense_process(i,context_length,length,token))\n",
        "data_list={'input_ids': data_list}"
      ],
      "metadata": {
        "id": "cj8N81UUH6Nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_list = Dataset.from_dict(data_list)\n",
        "data_list = DatasetDict({\"train\": data_list})\n",
        "data_list = data_list['train'].train_test_split(train_size=0.9,seed=23)"
      ],
      "metadata": {
        "id": "IEY1J1oD3TWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_funtion(element):\n",
        "  return {'input_ids': element['input_ids']}"
      ],
      "metadata": {
        "id": "WexQP3MgygHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_map=data_list.map(map_funtion)"
      ],
      "metadata": {
        "id": "THkF5g0lOgrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config=AutoConfig.from_pretrained(\n",
        "    'gpt2',\n",
        "    vocab_size=len(tokenizer),\n",
        "    n_ctx=context_length,\n",
        "    bos_token_id=tokenizer.bos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")"
      ],
      "metadata": {
        "id": "W7WTGtq_Kxm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=TFGPT2LMHeadModel(config)\n",
        "model(model.dummy_inputs)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "E8e8ck0bLwK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors=\"tf\")"
      ],
      "metadata": {
        "id": "_S3EeMSlMwFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_train_dataset = model.prepare_tf_dataset(\n",
        "    data_map[\"train\"],\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=True,\n",
        "    batch_size=16,\n",
        ")\n",
        "tf_eval_dataset = model.prepare_tf_dataset(\n",
        "    data_map[\"test\"],\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=False,\n",
        "    batch_size=16,\n",
        ")"
      ],
      "metadata": {
        "id": "kV4I66H2M6mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_loss(label, pred):\n",
        "    mask = label != 50256\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "    loss = loss_object(label, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss *= mask\n",
        "\n",
        "    loss = tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "VNJXmT2hApqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_accuracy(label, pred):\n",
        "    pred = tf.argmax(pred, axis=2)\n",
        "    label = tf.cast(label, pred.dtype)\n",
        "    match = label == pred\n",
        "\n",
        "    mask = label != 50256\n",
        "\n",
        "    match = match & mask\n",
        "\n",
        "    match = tf.cast(match, dtype=tf.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    return tf.reduce_sum(match) / tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "rrSPFvyDDb7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_learning_rate=0.00005\n",
        "decay_steps=2000\n",
        "decay_rate=0.96\n",
        "\n",
        "lr_schedule=ExponentialDecay(\n",
        "      initial_learning_rate,\n",
        "      decay_steps,\n",
        "      decay_rate,\n",
        "      staircase=True\n",
        "  )\n",
        "optimizer=Adam(lr_schedule)\n",
        "\n",
        "model.compile(optimizer=optimizer,metrics=['accuracy'])\n",
        "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
      ],
      "metadata": {
        "id": "_pWzvTceQIUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "XMVMe5W3Sh8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(tf_train_dataset,epochs=100,validation_data=tf_eval_dataset)"
      ],
      "metadata": {
        "id": "tcXVt7EsRWAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\n",
        "    \"text-generation\", model=model, tokenizer=tokenizer, device=0\n",
        ")"
      ],
      "metadata": {
        "id": "3joXtatZU6BH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt='I love you'\n",
        "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "dFdWxqrnU6pz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}